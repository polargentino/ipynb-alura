{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcd03bb",
   "metadata": {},
   "source": [
    "# Challenge completo con DeepSeek(15/07/2025): Pablo Matías Monsalvo\n",
    "## Primera parte de dos:\n",
    "### Challenge Telecom X: análisis de evasión de clientes\n",
    "####  03 Importancia del desafio\n",
    " Siguiente pregunta\n",
    "\n",
    "### Desafío Telecom X\n",
    "El desafío Telecom X ofrece una oportunidad única para aplicar habilidades esenciales de análisis de datos en un escenario de negocios real.\n",
    "\n",
    "### Aplicación práctica del conocimiento\n",
    "La limpieza y tratamiento de datos es una habilidad fundamental para cualquier analista de datos. La manipulación de grandes volúmenes de información exige la capacidad de identificar y corregir inconsistencias en los datos, como valores nulos, duplicados y datos fuera de estándar. Garantizar que los datos estén listos para el análisis es un paso esencial para obtener resultados precisos y confiables.\n",
    "\n",
    "El análisis exploratorio de datos (EDA) es una etapa crucial para comprender en profundidad los datos. La capacidad de aplicar estadísticas descriptivas y generar visualizaciones permite identificar patrones, tendencias y relaciones entre las variables. Esto ayuda a formular hipótesis y generar insights que pueden influir en decisiones estratégicas dentro de la empresa.\n",
    "\n",
    "Al participar en este desafío, aplicarás conocimientos esenciales para el análisis de grandes volúmenes de datos en un contexto real, donde tus hallazgos podrán impactar directamente en las estrategias de la empresa para mejorar el principal problema que están enfrentando.\n",
    "\n",
    "Este desafío no solo contribuye a tu crecimiento en el área de Data Science, sino que también ofrece la oportunidad de entender cómo la ciencia de datos puede aplicarse para resolver problemas reales que enfrentan las empresas en el mercado.\n",
    "\n",
    "## Trello:\n",
    "### 💡Acerca del desafío 💡\n",
    "Descripción\n",
    "#### Telecom X - Análisis de Evasión de Clientes\n",
    "Has sido contratado como asistente de análisis de datos en Telecom X y formarás parte del proyecto \"Churn de Clientes\". La empresa enfrenta una alta tasa de cancelaciones y necesita comprender los factores que llevan a la pérdida de clientes.\n",
    "\n",
    "Tu desafío será recopilar, procesar y analizar los datos, utilizando Python y sus principales bibliotecas para extraer información valiosa. A partir de tu análisis, el equipo de Data Science podrá avanzar en modelos predictivos y desarrollar estrategias para reducir la evasión.\n",
    "\n",
    "¿Qué vas a practicar?\n",
    "✅ Importar y manipular datos desde una API de manera eficiente.\n",
    "✅ Aplicar los conceptos de ETL (Extracción, Transformación y Carga) en la preparación de los datos.\n",
    "✅ Crear visualizaciones estratégicas para identificar patrones y tendencias.\n",
    "✅ Realizar un Análisis Exploratorio de Datos (EDA) y generar un informe con insights relevantes.\n",
    "\n",
    "¡Ahora es tu turno! 🚀 Usa tus conocimientos para transformar datos en información estratégica y ayudar a Telecom X a retener más clientes.\n",
    "\n",
    "## 1-Extracción de datos:\n",
    "Descripción\n",
    "Para iniciar tu análisis, necesitarás importar los datos de la API de Telecom X. Estos datos están disponibles en formato JSON y contienen información esencial sobre los clientes, incluyendo datos demográficos, tipo de servicio contratado y estado de evasión.\n",
    "\n",
    "📌 Enlace de la API:\n",
    "🔗 challenge2-data-science-LATAM/TelecomX_Data.json at main · ingridcristh/challenge2-data-science-LATAM\n",
    "\n",
    "🔗GitHub - ingridcristh/challenge2-data-science-LATAM\n",
    "\n",
    "¿Qué debes hacer?\n",
    "✅ Cargar los datos directamente desde la API utilizando Python.\n",
    "✅ Convertir los datos a un DataFrame de Pandas para facilitar su manipulación.\n",
    "\n",
    "Este es el primer paso para transformar los datos en información valiosa. ¿Listo para programar? 🚀\n",
    "\n",
    "## 2 - Conoce el conjunto de datos\n",
    "Descripción\n",
    "Ahora que has extraído los datos, es fundamental comprender la estructura del dataset y el significado de sus columnas. Esta etapa te ayudará a identificar qué variables son más relevantes para el análisis de evasión de clientes.\n",
    "\n",
    "📌 Para facilitar este proceso, hemos creado un diccionario de datos con la descripción de cada columna. Aunque no es obligatorio utilizarlo, puede ayudarte a comprender mejor la información disponible.\n",
    "\n",
    "🔗 Enlace al diccionario y a la API\n",
    "\n",
    "¿Qué debes hacer?\n",
    "✅ Explorar las columnas del dataset y verificar sus tipos de datos.\n",
    "✅ Consultar el diccionario para comprender mejor el significado de las variables.\n",
    "✅ Identificar las columnas más relevantes para el análisis de evasión.\n",
    "\n",
    "📌 Tips:\n",
    "🔗 Documentación de DataFrame.info()\n",
    "🔗 Documentación de DataFrame.dtypes\n",
    "\n",
    "## 3 - Comprobación de incoherencias en los datos\n",
    "Descripción\n",
    "En este paso, verifica si hay problemas en los datos que puedan afectar el análisis. Presta atención a valores ausentes, duplicados, errores de formato e inconsistencias en las categorías. Este proceso es esencial para asegurarte de que los datos estén listos para las siguientes etapas.\n",
    "\n",
    "📌 Tips:\n",
    "\n",
    "🔗 Documentación de pandas.unique()\n",
    "🔗 Documentación de pandas.Series.dt.normalize()\n",
    "\n",
    "## 4 - Manejo de inconsistencias\n",
    "Descripción\n",
    "Ahora que has identificado las inconsistencias, es momento de aplicar las correcciones necesarias. Ajusta los datos para asegurarte de que estén completos y coherentes, preparándolos para las siguientes etapas del análisis.\n",
    "\n",
    "📌 Tips:\n",
    "\n",
    "🔗 Manipulación de strings en pandas: lower, replace, startswith y contains | Alura Cursos Online\n",
    "\n",
    "\n",
    "## 5 - Columna de cuentas diarias\n",
    "Descripción\n",
    "Ahora que los datos están limpios, es momento de crear la columna \"Cuentas_Diarias\". Utiliza la facturación mensual para calcular el valor diario, proporcionando una visión más detallada del comportamiento de los clientes a lo largo del tiempo.\n",
    "\n",
    "📌 Esta columna te ayudará a profundizar en el análisis y a obtener información valiosa para las siguientes etapas.\n",
    "\n",
    "## 6 - Estandarización y transformación de datos (opcional)\n",
    "Descripción\n",
    "La estandarización y transformación de datos es una etapa opcional, pero altamente recomendada, ya que busca hacer que la información sea más consistente, comprensible y adecuada para el análisis. Durante esta fase, por ejemplo, puedes convertir valores textuales como \"Sí\" y \"No\" en valores binarios (1 y 0), lo que facilita el procesamiento matemático y la aplicación de modelos analíticos.\n",
    "\n",
    "Además, traducir o renombrar columnas y datos hace que la información sea más accesible y fácil de entender, especialmente cuando se trabaja con fuentes externas o términos técnicos. Aunque no es un paso obligatorio, puede mejorar significativamente la claridad y comunicación de los resultados, facilitando la interpretación y evitando confusiones, especialmente al compartir información con stakeholders no técnicos.\n",
    "\n",
    "## 7 - Carga y análisis(L - Load & Analysis)\n",
    "### Análisis Descriptivo\n",
    "Descripción\n",
    "Para comenzar, realiza un análisis descriptivo de los datos, calculando métricas como media, mediana, desviación estándar y otras medidas que ayuden a comprender mejor la distribución y el comportamiento de los clientes.\n",
    "\n",
    "📌 Consejos:\n",
    "\n",
    "🔗 Documentación de DataFrame.describe()\n",
    "\n",
    "## 8 - Distribución de evasión\n",
    "Descripción\n",
    "En este paso, el objetivo es comprender cómo está distribuida la variable \"churn\" (evasión) entre los clientes. Utiliza gráficos para visualizar la proporción de clientes que permanecieron y los que se dieron de baja.\n",
    "\n",
    "## 9 - Recuento de evasión por variables categóricas\n",
    "Descripción\n",
    "Ahora, exploraremos cómo se distribuye la evasión según variables categóricas, como género, tipo de contrato, método de pago, entre otras.\n",
    "\n",
    "Este análisis puede revelar patrones interesantes, por ejemplo, si los clientes de ciertos perfiles tienen una mayor tendencia a cancelar el servicio, lo que ayudará a orientar acciones estratégicas.\n",
    "\n",
    "## 10 - Conteo de evasión por variables numéricas\n",
    "Descripción\n",
    "En este paso, explora cómo las variables numéricas, como \"total gastado\" o \"tiempo de contrato\", se distribuyen entre los clientes que cancelaron (evasión) y los que no cancelaron.\n",
    "\n",
    "Este análisis ayuda a entender si ciertos valores numéricos están más asociados con la evasión, proporcionando insights sobre los factores que influyen en el comportamiento de los clientes.\n",
    "\n",
    "## 11 - Informe final\n",
    "Descripción\n",
    "Finaliza el desafío elaborando un informe dentro del mismo notebook que resuma todo el trabajo realizado. El informe debe incluir:\n",
    "\n",
    "🔹 Introducción: Explica el objetivo del análisis y el problema de evasión de clientes (Churn).\n",
    "\n",
    "🔹 Limpieza y Tratamiento de Datos: Describe los pasos realizados para importar, limpiar y procesar los datos.\n",
    "\n",
    "🔹 Análisis Exploratorio de Datos: Presenta los análisis realizados, incluyendo gráficos y visualizaciones para identificar patrones.\n",
    "\n",
    "🔹 Conclusiones e Insights: Resume los principales hallazgos y cómo estos datos pueden ayudar a reducir la evasión.\n",
    "\n",
    "🔹 Recomendaciones: Ofrece sugerencias estratégicas basadas en tu análisis.\n",
    "\n",
    "Asegúrate de que el informe esté bien estructurado, claro y respaldado por visualizaciones que refuercen tus conclusiones. 🚀\n",
    "\n",
    "\n",
    "## 12 - ¡Extra! Análisis de correlación entre variables\n",
    "Descripción\n",
    "Esta actividad es un extra, por lo tanto es OPCIONAL.\n",
    "\n",
    "Como un paso adicional, puedes explorar la correlación entre diferentes variables del dataset. Esto puede ayudar a identificar qué factores tienen mayor relación con la evasión de clientes, como:\n",
    "\n",
    "🔹 La relación entre la cuenta diaria y la evasión.\n",
    "🔹 Cómo la cantidad de servicios contratados afecta la probabilidad de churn.\n",
    "\n",
    "Puedes usar la función corr() de Pandas para calcular las correlaciones y visualizar los resultados con gráficos de dispersión o matrices de correlación.\n",
    "\n",
    "Este análisis adicional puede proporcionar insights valiosos para la creación de modelos predictivos más robustos. 🚀\n",
    "\n",
    "## 13 - 📖 README 📖\n",
    "Descripción\n",
    "El README es un elemento clave en cualquier proyecto de desarrollo, ya que proporciona una descripción clara y detallada del propósito, la estructura y el uso del código.\n",
    "\n",
    "Cuando participas en un proceso de selección, el README es imprescindible para comunicar cómo utilizar el proyecto.\n",
    "\n",
    "Este archivo, con la extensión .md (Markdown), es el punto de referencia inicial para cualquiera que quiera entender y trabajar con su código.\n",
    "\n",
    "Un buen README incluye información sobre la instalación, dependencias, cómo ejecutar el proyecto y posibles problemas o soluciones.\n",
    "\n",
    "Un README bien estructurado facilita que otros desarrolladores comprendan el proyecto.\n",
    "\n",
    "Aquí hay un artículo con los pasos para crear un README increíble:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d77ab5",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34a024",
   "metadata": {},
   "source": [
    "# Segunda parte: 2/2\n",
    "## Telecom X – Parte 2: Predicción de Cancelación (Churn)\n",
    "📣 Historia del Desafío\n",
    "\n",
    "¡Felicidades! 🎉 Has sido promovido después de tu excelente desempeño en el análisis exploratorio de la cancelación de clientes en Telecom X. Tu dedicación, claridad al comunicar los datos y visión estratégica marcaron la diferencia.\n",
    "\n",
    "Ahora, ¡has sido invitado oficialmente a formar parte del equipo de Machine Learning de la empresa!\n",
    "\n",
    "🎯 Misión\n",
    "\n",
    "Tu nueva misión es desarrollar modelos predictivos capaces de prever qué clientes tienen mayor probabilidad de cancelar sus servicios.\n",
    "\n",
    "La empresa quiere anticiparse al problema de la cancelación, y te corresponde a ti construir un pipeline robusto para esta etapa inicial de modelado.\n",
    "\n",
    "🧠 Objetivos del Desafío\n",
    "\n",
    "Preparar los datos para el modelado (tratamiento, codificación, normalización).\n",
    "\n",
    "Realizar análisis de correlación y selección de variables.\n",
    "\n",
    "Entrenar dos o más modelos de clasificación.\n",
    "\n",
    "Evaluar el rendimiento de los modelos con métricas.\n",
    "\n",
    "Interpretar los resultados, incluyendo la importancia de las variables.\n",
    "\n",
    "Crear una conclusión estratégica señalando los principales factores que influyen en la cancelación.\n",
    "\n",
    "🧰 Lo que vas a practicar\n",
    "\n",
    "✅ Preprocesamiento de datos para Machine Learning\n",
    "✅ Construcción y evaluación de modelos predictivos\n",
    "✅ Interpretación de resultados y entrega de insights\n",
    "✅ Comunicación técnica con enfoque estratégico\n",
    "\n",
    "🚀 Ahora eres: Analista Junior de Machine Learning\n",
    "\n",
    "Telecom X confía en tu entrega para dar los próximos pasos hacia una solución de inteligencia predictiva eficaz. ¡Buena suerte!\n",
    "\n",
    "\n",
    "## 1- Extracción del Archivo Tratado\n",
    "Descripción\n",
    "Carga el archivo CSV que contiene los datos tratados anteriormente.\n",
    "📂 Atención: Utiliza el mismo archivo que limpiaste y organizaste en la Parte 1 del desafío Telecom X. Debe contener solo las columnas relevantes, ya con los datos corregidos y estandarizados.\n",
    "\n",
    "## 2 - Eliminación de Columnas Irrelevantes\n",
    "Descripción\n",
    "Elimina columnas que no aportan valor al análisis o a los modelos predictivos, como identificadores únicos (por ejemplo, el ID del cliente). Estas columnas no ayudan en la predicción de la cancelación y pueden incluso perjudicar el desempeño de los modelos.\n",
    "\n",
    "## 3 - Encoding\n",
    "Descripción\n",
    "Transforma las variables categóricas a formato numérico para hacerlas compatibles con los algoritmos de machine learning. Utiliza un método de codificación adecuado, como one-hot encoding.\n",
    "\n",
    "🔎 Sugerencia:\n",
    "Puedes consultar este artículo para entender mejor cuándo usar get_dummies o OneHotEncoder:\n",
    "Artículo en Alura sobre codificación categórica\n",
    "\n",
    "## 4 - Verificación de la Proporción de Cancelación (Churn)\n",
    "Descripción\n",
    "Calcula la proporción de clientes que cancelaron en relación con los que permanecieron activos. Evalúa si existe un desbalance entre las clases, ya que esto puede impactar en los modelos predictivos y en el análisis de los resultados.\n",
    "\n",
    "🔎 Sugerencia:\n",
    "Puedes usar value_counts() de pandas para obtener esta proporción:\n",
    "Documentación oficial de value_counts()\n",
    "\n",
    "## 5 - Balanceo de Clases (opcional)\n",
    "Descripción\n",
    "Si deseas profundizar en el análisis, aplica técnicas de balanceo como undersampling o oversampling. En situaciones de fuerte desbalanceo, herramientas como SMOTE pueden ser útiles para generar ejemplos sintéticos de la clase minoritaria.\n",
    "\n",
    "🔎 Sugerencia:\n",
    "Puedes leer más sobre cómo manejar el desbalanceo de clases en este artículo:\n",
    "Artículo en Alura sobre desbalanceo de datos\n",
    "\n",
    "## 6 - Normalización o Estandarización (si es necesario)\n",
    "Evalúa la necesidad de normalizar o estandarizar los datos, según los modelos que se aplicarán. Modelos basados en distancia, como KNN, SVM, Regresión Logística y Redes Neuronales, requieren este preprocesamiento. Por otro lado, modelos basados en árboles, como Decision Tree, Random Forest y XGBoost, no son sensibles a la escala de los datos.\n",
    "\n",
    "🔎 Sugerencia:\n",
    "Puedes leer más sobre normalización y estandarización de datos en este artículo:\n",
    "Artículo en Medium sobre normalización y estandarización en Machine Learning\n",
    "\n",
    "## 7 - 🎯 Correlación y Selección de Variables\n",
    "### Análisis de Correlación\n",
    "#### Descripción\n",
    "Visualiza la matriz de correlación para identificar relaciones entre las variables numéricas. Presta especial atención a las variables que muestran una mayor correlación con la cancelación, ya que estas pueden ser fuertes candidatas para el modelo predictivo.\n",
    "\n",
    "## 8 - Análisis Dirigido\n",
    "Descripción\n",
    "Investiga cómo variables específicas se relacionan con la cancelación, tales como:\n",
    "\n",
    "Tiempo de contrato × Cancelación\n",
    "\n",
    "Gasto total × Cancelación\n",
    "\n",
    "Utiliza gráficos como boxplots o scatter plots para visualizar patrones y posibles tendencias.\n",
    "\n",
    "## 9 - 🤖 Modelado Predictivo\n",
    "### Separación de Datos\n",
    "Descripción\n",
    "Divide el conjunto de datos en entrenamiento y prueba para evaluar el rendimiento del modelo. Una división común es 70% para entrenamiento y 30% para prueba, o 80/20, dependiendo del tamaño de la base de datos.\n",
    "\n",
    "## 10 - Creación de Modelos\n",
    "Descripción\n",
    "Crea al menos dos modelos diferentes para predecir la cancelación de clientes.\n",
    "\n",
    "Un modelo puede requerir normalización, como Regresión Logística o KNN.\n",
    "\n",
    "El otro modelo puede no requerir normalización, como Árbol de Decisión o Random Forest.\n",
    "\n",
    "💡 La decisión de aplicar o no la normalización depende de los modelos seleccionados. Ambos modelos pueden ser creados sin normalización, pero también es una opción combinar modelos con y sin normalización.\n",
    "\n",
    "#### Justificación:\n",
    "\n",
    "* Regresión Logística / KNN: Estos modelos son sensibles a la escala de los datos, por lo que la normalización es importante para que los coeficientes o las distancias se calculen correctamente.\n",
    "\n",
    "* Árbol de Decisión / Random Forest: Estos modelos no dependen de la escala de los datos, por lo que no es necesario aplicar normalización.\n",
    "\n",
    "Si decides normalizar los datos, deberías explicar cómo esta etapa asegura que los modelos basados en distancia o en optimización de parámetros no se vean sesgados por la magnitud de las variables.\n",
    "\n",
    "\n",
    "## 11 - Evaluación de los Modelos\n",
    "Descripción\n",
    "Evalúa cada modelo utilizando las siguientes métricas:\n",
    "\n",
    "* Exactitud (Acurácia)\n",
    "\n",
    "* Precisión\n",
    "\n",
    "* Recall\n",
    "\n",
    "* F1-score\n",
    "\n",
    "* Matriz de confusión\n",
    "\n",
    "Después, realiza un análisis crítico y compara los modelos:\n",
    "\n",
    "* ¿Cuál modelo tuvo el mejor desempeño?\n",
    "\n",
    "* ¿Algún modelo presentó overfitting o underfitting? Si es así, considera las posibles causas y ajustes:\n",
    "\n",
    "**Overfitting**: Cuando el modelo aprende demasiado sobre los datos de entrenamiento, perdiendo la capacidad de generalizar a nuevos datos. Considera reducir la complejidad del modelo o aumentar los datos de entrenamiento.\n",
    "\n",
    "**Underfitting**: Cuando el modelo no captura bien las tendencias de los datos, lo que indica que es demasiado simple. Intenta aumentar la complejidad del modelo o ajustar sus parámetros.\n",
    "\n",
    "## 📋 12 - Interpretación y Conclusiones\n",
    "### Análisis de la Importancia de las Variables\n",
    "Descripción\n",
    "Después de elegir los modelos, realiza el análisis de las variables más relevantes para la predicción de la cancelación:\n",
    "\n",
    "* Regresión Logística: Investiga los coeficientes de las variables, que muestran su contribución a la predicción de cancelación.\n",
    "\n",
    "* KNN (K-Nearest Neighbors): Observa cómo los vecinos más cercanos influyen en la decisión de clasificación. Las variables más impactantes pueden ser aquellas que más contribuyen a la proximidad entre los puntos de datos.\n",
    "\n",
    "* Random Forest: Utiliza la importancia de las variables proporcionada por el modelo. Random Forest calcula la importancia basándose en cómo cada variable contribuye a la reducción de la impureza durante las divisiones de los árboles.\n",
    "\n",
    "* SVM (Support Vector Machine): En el SVM, las variables más relevantes son aquellas que influyen en la frontera de decisión entre las clases. Puedes analizar los coeficientes de los vectores de soporte para entender qué variables tienen mayor impacto.\n",
    "\n",
    "* Otros Modelos: Dependiendo del modelo elegido, considera el análisis de métricas específicas para comprender la relevancia de las variables. Por ejemplo, coeficientes en modelos lineales, pesos en redes neuronales, o la importancia relativa en técnicas de boosting (como XGBoost).\n",
    "\n",
    "## 13 - Conclusión\n",
    "Descripción\n",
    "Elaboren un informe detallado, destacando los factores que más influyen en la cancelación, basándose en las variables seleccionadas y en el rendimiento de cada modelo.\n",
    "\n",
    "Identifiquen los principales factores que afectan la cancelación de clientes y propongan estrategias de retención basadas en los resultados obtenidos.\n",
    "\n",
    "## ruta:\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Ruta al archivo local\n",
    "ruta_archivo = '../data/TelecomX_Data.json'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
